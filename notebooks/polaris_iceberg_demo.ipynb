{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85e575d1-6292-41c4-a1bc-c486b7c1d7ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CELL 0: ENVIRONMENT VALIDATION\n",
    "# ============================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"üîß STEP 1: VALIDATING DATABRICKS ENVIRONMENT\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Check Java version\n",
    "java_version = spark.sparkContext._jvm.java.lang.System.getProperty(\"java.version\")\n",
    "print(f\"\\n‚úÖ Java Version: {java_version}\")\n",
    "\n",
    "# Check Spark version\n",
    "spark_version = spark.version\n",
    "print(f\"‚úÖ Spark Version: {spark_version}\")\n",
    "\n",
    "# Check Python version\n",
    "import sys\n",
    "python_version = sys.version.split()[0]\n",
    "print(f\"‚úÖ Python Version: {python_version}\")\n",
    "\n",
    "# Verify Spark is working\n",
    "try:\n",
    "    test_df = spark.range(1, 3)\n",
    "    test_count = test_df.count()\n",
    "    print(f\"‚úÖ Spark Context: Active (test count = {test_count})\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Spark Context Error: {e}\")\n",
    "    \n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úÖ ENVIRONMENT VALIDATION COMPLETE\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c825af9b-2b8c-4c65-b910-386b786719f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CELL 1: SNOWFLAKE POLARIS CONFIGURATION\n",
    "# ============================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"üîß STEP 2: CONFIGURING SNOWFLAKE POLARIS CATALOG CONNECTION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Snowflake Polaris credentials - UPDATE WITH YOUR VALUES\n",
    "ACCOUNTADMIN_PAT = \"YOUR_SNOWFLAKE_PAT_TOKEN_HERE\"  # Get from: Snowflake ‚Üí Profile ‚Üí Security ‚Üí Tokens\n",
    "SNOWFLAKE_ACCOUNT = \"YOUR-SNOWFLAKE-ACCOUNT\"  # Format: ORGNAME-ACCOUNTNAME (no underscores!)\n",
    "DATABASE_NAME = \"DEMO_TESTDB\"\n",
    "horizon_role = \"session:role:ACCOUNTADMIN\"\n",
    "\n",
    "# Construct Polaris API endpoint\n",
    "account_uri = f\"https://{SNOWFLAKE_ACCOUNT}.snowflakecomputing.com/polaris/api/catalog\"\n",
    "\n",
    "print(f\"\\nüìç Configuration Details:\")\n",
    "print(f\"   Account: {SNOWFLAKE_ACCOUNT}\")\n",
    "print(f\"   Database: {DATABASE_NAME}\")\n",
    "print(f\"   Polaris URI: {account_uri}\")\n",
    "print(f\"   Role: ACCOUNTADMIN\")\n",
    "\n",
    "# Configure Spark to use Snowflake Polaris as Iceberg catalog\n",
    "print(f\"\\n‚öôÔ∏è  Configuring Spark catalog 'hz' for Polaris...\")\n",
    "\n",
    "try:\n",
    "    # Clear any existing catalog configuration first\n",
    "    try:\n",
    "        spark.conf.unset(\"spark.sql.catalog.hz\")\n",
    "        spark.conf.unset(\"spark.sql.catalog.hz.type\")\n",
    "    except:\n",
    "        pass  # Ignore if not set\n",
    "    \n",
    "    # Core Polaris/Iceberg configuration\n",
    "    # Use catalog-impl approach (not type) to avoid conflicts\n",
    "    spark.conf.set(\"spark.sql.catalog.hz\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "    spark.conf.set(\"spark.sql.catalog.hz.catalog-impl\", \"org.apache.iceberg.rest.RESTCatalog\")\n",
    "    spark.conf.set(\"spark.sql.catalog.hz.uri\", account_uri)\n",
    "    spark.conf.set(\"spark.sql.catalog.hz.warehouse\", DATABASE_NAME)\n",
    "    spark.conf.set(\"spark.sql.catalog.hz.credential\", ACCOUNTADMIN_PAT)\n",
    "    spark.conf.set(\"spark.sql.catalog.hz.scope\", horizon_role)\n",
    "    spark.conf.set(\"spark.sql.catalog.hz.header.X-Snowflake-Authorization-Token-Type\", \"PROGRAMMATIC_ACCESS_TOKEN\")\n",
    "    spark.conf.set(\"spark.sql.iceberg.vectorization.enabled\", \"false\")\n",
    "    \n",
    "    print(\"‚úÖ Spark catalog configuration set successfully\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Configuration Error: {e}\")\n",
    "    raise\n",
    "\n",
    "# Verify configuration was applied\n",
    "print(f\"\\nüîç Verifying configuration...\")\n",
    "catalog_uri = spark.conf.get(\"spark.sql.catalog.hz.uri\")\n",
    "catalog_warehouse = spark.conf.get(\"spark.sql.catalog.hz.warehouse\")\n",
    "print(f\"   ‚úÖ Catalog URI: {catalog_uri}\")\n",
    "print(f\"   ‚úÖ Warehouse: {catalog_warehouse}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úÖ POLARIS CONFIGURATION COMPLETE\")\n",
    "print(\"=\" * 80)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "492621e2-2edb-4899-9c27-b8869f6ae1d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CELL 2: TEST CONNECTION & DISCOVERY\n",
    "# ============================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"üîß STEP 3: TESTING POLARIS CONNECTION & DISCOVERING SCHEMAS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Test 1: List namespaces (schemas)\n",
    "print(\"\\nüìã Test 1: Listing namespaces in Polaris catalog...\")\n",
    "try:\n",
    "    namespaces_df = spark.sql(\"SHOW NAMESPACES IN hz\")\n",
    "    namespace_count = namespaces_df.count()\n",
    "    \n",
    "    print(f\"‚úÖ Successfully connected to Polaris catalog!\")\n",
    "    print(f\"‚úÖ Found {namespace_count} namespace(s):\\n\")\n",
    "    namespaces_df.show(truncate=False)\n",
    "    \n",
    "    # Store namespaces for validation\n",
    "    # Note: The column might be 'databaseName' or 'namespace' depending on Iceberg version\n",
    "    try:\n",
    "        namespaces = [row.databaseName for row in namespaces_df.collect()]\n",
    "    except:\n",
    "        namespaces = [row.namespace for row in namespaces_df.collect()]\n",
    "    \n",
    "    # Verify PUBLIC schema exists\n",
    "    if \"PUBLIC\" in namespaces:\n",
    "        print(\"‚úÖ PUBLIC schema found (expected)\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  PUBLIC schema not found - check Snowflake setup\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå CONNECTION FAILED: {e}\")\n",
    "    print(\"\\nüí° Troubleshooting:\")\n",
    "    print(\"   1. Verify Polaris catalog is enabled in Snowflake\")\n",
    "    print(\"   2. Check your PAT token is valid\")\n",
    "    print(\"   3. Ensure database DEMO_TESTDB exists\")\n",
    "    print(\"   4. Verify Iceberg libraries are installed on cluster\")\n",
    "    raise\n",
    "\n",
    "# Test 2: List tables in PUBLIC schema\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üìã Test 2: Listing tables in PUBLIC schema...\")\n",
    "try:\n",
    "    tables_df = spark.sql(\"SHOW TABLES IN hz.PUBLIC\")\n",
    "    table_count = tables_df.count()\n",
    "    \n",
    "    print(f\"‚úÖ Found {table_count} table(s) in PUBLIC schema:\\n\")\n",
    "    tables_df.show(truncate=False)\n",
    "    \n",
    "    # Store table names for validation\n",
    "    # Get the first row to check available columns\n",
    "    if table_count > 0:\n",
    "        first_row = tables_df.first()\n",
    "        # Try different possible column names\n",
    "        try:\n",
    "            tables = [row.tableName for row in tables_df.collect()]\n",
    "        except:\n",
    "            try:\n",
    "                tables = [row.name for row in tables_df.collect()]\n",
    "            except:\n",
    "                # Just get all column values from first column\n",
    "                tables = [row[0] for row in tables_df.collect()]\n",
    "        \n",
    "        # Verify USER_INFO table exists\n",
    "        tables_upper = [t.upper() if t else \"\" for t in tables]\n",
    "        if \"USER_INFO\" in tables_upper:\n",
    "            print(\"‚úÖ USER_INFO table found (target table confirmed)\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è  USER_INFO table not found - check Snowflake table creation\")\n",
    "            print(f\"   Available tables: {tables}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  No tables found in PUBLIC schema\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå DISCOVERY FAILED: {e}\")\n",
    "    raise\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úÖ CONNECTION TEST & DISCOVERY COMPLETE\")\n",
    "print(\"=\" * 80)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b6b1097-97fc-4897-9744-5ad3bbd55ada",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CELL 3: QUERY USER_INFO TABLE\n",
    "# ============================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"üîß STEP 4: READING USER_INFO TABLE FROM SNOWFLAKE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Query the USER_INFO table\n",
    "print(\"\\nüë• Querying USER_INFO table via Polaris catalog...\")\n",
    "\n",
    "user_info_df = spark.sql(\"SELECT * FROM hz.PUBLIC.USER_INFO\")\n",
    "\n",
    "print(\"\\nüìä USER_INFO Table:\")\n",
    "user_info_df.show(truncate=False)\n",
    "\n",
    "# Show row count\n",
    "row_count = user_info_df.count()\n",
    "print(f\"\\n‚úÖ Total rows: {row_count}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úÖ QUERY COMPLETE\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CELL 4: FINAL VALIDATION & DEMO SUMMARY\n",
    "# ============================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"üéØ FINAL VALIDATION & DEMO SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Summary of what was accomplished\n",
    "print(\"\\n‚úÖ DEMO COMPLETED SUCCESSFULLY!\\n\")\n",
    "\n",
    "print(\"üìã What We Accomplished:\")\n",
    "print(\"   ‚úÖ Step 1: Validated Databricks environment\")\n",
    "print(\"   ‚úÖ Step 2: Configured Snowflake Polaris catalog connection\")\n",
    "print(\"   ‚úÖ Step 3: Tested connection and discovered schemas/tables\")\n",
    "print(\"   ‚úÖ Step 4: Read and validated USER_INFO table\")\n",
    "\n",
    "print(\"\\nüèóÔ∏è  Architecture Demonstrated:\")\n",
    "print(\"   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\")\n",
    "print(\"   ‚îÇ         SNOWFLAKE                        ‚îÇ\")\n",
    "print(\"   ‚îÇ  Database: DEMO_TESTDB                   ‚îÇ\")\n",
    "print(\"   ‚îÇ  Table: USER_INFO (Iceberg format)       ‚îÇ\")\n",
    "print(\"   ‚îÇ  Polaris Catalog (REST API)              ‚îÇ\")\n",
    "print(\"   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\")\n",
    "print(\"                  ‚îÇ\")\n",
    "print(\"                  ‚îÇ REST API over HTTPS\")\n",
    "print(\"                  ‚îÇ (Iceberg protocol)\")\n",
    "print(\"                  ‚îÇ\")\n",
    "print(\"   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\")\n",
    "print(\"   ‚îÇ         DATABRICKS                       ‚îÇ\")\n",
    "print(\"   ‚îÇ  Spark + Iceberg libraries               ‚îÇ\")\n",
    "print(\"   ‚îÇ  Querying remote Iceberg tables          ‚îÇ\")\n",
    "print(\"   ‚îÇ  No data duplication!                    ‚îÇ\")\n",
    "print(\"   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\")\n",
    "\n",
    "print(\"\\nüîë Key Capabilities Shown:\")\n",
    "print(\"   ‚Ä¢ Data Federation: Query Snowflake data from Databricks\")\n",
    "print(\"   ‚Ä¢ Open Standards: Using Apache Iceberg format\")\n",
    "print(\"   ‚Ä¢ No ETL: Direct access without copying data\")\n",
    "print(\"   ‚Ä¢ Polaris Catalog: Snowflake's REST-based catalog service\")\n",
    "\n",
    "print(\"\\nüìä Table Statistics:\")\n",
    "try:\n",
    "    final_count = spark.sql(\"SELECT COUNT(*) as count FROM hz.PUBLIC.USER_INFO\").collect()[0]['count']\n",
    "    print(f\"   Total rows in USER_INFO: {final_count}\")\n",
    "    \n",
    "    final_sample = spark.sql(\"SELECT * FROM hz.PUBLIC.USER_INFO LIMIT 3\")\n",
    "    print(\"\\n   Sample data:\")\n",
    "    final_sample.show(truncate=False)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"   Could not fetch final stats: {e}\")\n",
    "\n",
    "print(\"\\nüöÄ Next Steps - Phase 2:\")\n",
    "print(\"   1. Add more tables (CUSTOMERS, ORDERS, PRODUCTS)\")\n",
    "print(\"   2. Set up Snowflake Cortex Analyst (semantic model)\")\n",
    "print(\"   3. Create Cortex Agent for natural language queries\")\n",
    "print(\"   4. Build demo presentation and talking points\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úÖ DEMO READY FOR PRESENTATION!\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create a flag to indicate success\n",
    "print(\"\\nüíæ Saving connection state for future use...\")\n",
    "spark.conf.set(\"demo.polaris.connection.validated\", \"true\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CELL 5: TROUBLESHOOTING & DIAGNOSTICS (Optional)\n",
    "# ============================================\n",
    "# Run this cell only if you encounter errors in previous cells\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"üîß TROUBLESHOOTING & DIAGNOSTICS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nüîç Diagnostic Information:\\n\")\n",
    "\n",
    "# Check 1: Verify Iceberg library is installed\n",
    "print(\"1Ô∏è‚É£ Checking for Iceberg libraries...\")\n",
    "try:\n",
    "    iceberg_class = spark.sparkContext._jvm.org.apache.iceberg.spark.SparkCatalog\n",
    "    print(\"   ‚úÖ Iceberg libraries are installed\")\n",
    "except Exception as e:\n",
    "    print(\"   ‚ùå Iceberg libraries NOT found!\")\n",
    "    print(\"   üí° Install: org.apache.iceberg:iceberg-spark-runtime-3.4_2.12:1.4.3\")\n",
    "    print(f\"   Error: {e}\")\n",
    "\n",
    "# Check 2: Verify catalog configuration\n",
    "print(\"\\n2Ô∏è‚É£ Checking catalog configuration...\")\n",
    "try:\n",
    "    catalog_impl = spark.conf.get(\"spark.sql.catalog.hz.catalog-impl\")\n",
    "    catalog_uri = spark.conf.get(\"spark.sql.catalog.hz.uri\")\n",
    "    print(f\"   ‚úÖ Catalog implementation: {catalog_impl}\")\n",
    "    print(f\"   ‚úÖ Catalog URI: {catalog_uri}\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Catalog not configured: {e}\")\n",
    "\n",
    "# Check 3: Network connectivity test\n",
    "print(\"\\n3Ô∏è‚É£ Testing network connectivity to Snowflake...\")\n",
    "try:\n",
    "    import urllib.request\n",
    "    import ssl\n",
    "    \n",
    "    # Test basic HTTPS connectivity to Snowflake\n",
    "    context = ssl.create_default_context()\n",
    "    url = f\"https://{SNOWFLAKE_ACCOUNT}.snowflakecomputing.com\"\n",
    "    \n",
    "    try:\n",
    "        req = urllib.request.Request(url, method='HEAD')\n",
    "        response = urllib.request.urlopen(req, context=context, timeout=10)\n",
    "        print(f\"   ‚úÖ Can reach Snowflake at {url}\")\n",
    "        print(f\"   HTTP Status: {response.status}\")\n",
    "    except Exception as net_err:\n",
    "        print(f\"   ‚ö†Ô∏è  Network issue: {net_err}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"   ‚ö†Ô∏è  Could not test network: {e}\")\n",
    "\n",
    "# Check 4: PAT token format validation\n",
    "print(\"\\n4Ô∏è‚É£ Validating PAT token format...\")\n",
    "try:\n",
    "    if ACCOUNTADMIN_PAT and len(ACCOUNTADMIN_PAT) > 50:\n",
    "        print(\"   ‚úÖ PAT token appears to be set (length > 50 chars)\")\n",
    "        # Check if it looks like a JWT\n",
    "        if ACCOUNTADMIN_PAT.count('.') == 2:\n",
    "            print(\"   ‚úÖ PAT token format looks like a JWT (3 parts)\")\n",
    "        else:\n",
    "            print(\"   ‚ö†Ô∏è  PAT token doesn't look like a JWT\")\n",
    "    else:\n",
    "        print(\"   ‚ùå PAT token not set or too short\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå PAT token validation failed: {e}\")\n",
    "\n",
    "# Check 5: List all Spark catalog configurations\n",
    "print(\"\\n5Ô∏è‚É£ All Spark catalog configurations:\")\n",
    "try:\n",
    "    all_configs = spark.sparkContext.getConf().getAll()\n",
    "    catalog_configs = [conf for conf in all_configs if 'catalog.hz' in conf[0]]\n",
    "    \n",
    "    if catalog_configs:\n",
    "        for key, value in catalog_configs:\n",
    "            # Mask the credential\n",
    "            if 'credential' in key:\n",
    "                value = value[:20] + \"...\" if len(value) > 20 else \"***\"\n",
    "            print(f\"   {key} = {value}\")\n",
    "    else:\n",
    "        print(\"   ‚ö†Ô∏è  No catalog configurations found\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Could not list configurations: {e}\")\n",
    "\n",
    "# Check 6: Common error patterns\n",
    "print(\"\\n6Ô∏è‚É£ Common Issues & Solutions:\")\n",
    "print(\"   ‚ùå 'Cannot find catalog plugin' ‚Üí Install Iceberg libraries\")\n",
    "print(\"   ‚ùå 'RESTException' ‚Üí Check Polaris is enabled in Snowflake\")\n",
    "print(\"   ‚ùå '401 Unauthorized' ‚Üí Verify PAT token is valid\")\n",
    "print(\"   ‚ùå '404 Not Found' ‚Üí Check database name and Snowflake account\")\n",
    "print(\"   ‚ùå 'Connection refused' ‚Üí Network/firewall issues\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úÖ DIAGNOSTICS COMPLETE\")\n",
    "print(\"=\" * 80)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Storage for Iceberg demo - trial account",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
