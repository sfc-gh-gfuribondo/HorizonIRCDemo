{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85e575d1-6292-41c4-a1bc-c486b7c1d7ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CELL 0: ENVIRONMENT VALIDATION\n",
    "# ============================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"üîß STEP 1: VALIDATING DATABRICKS ENVIRONMENT\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Check Java version\n",
    "java_version = spark.sparkContext._jvm.java.lang.System.getProperty(\"java.version\")\n",
    "print(f\"\\n‚úÖ Java Version: {java_version}\")\n",
    "\n",
    "# Check Spark version\n",
    "spark_version = spark.version\n",
    "print(f\"‚úÖ Spark Version: {spark_version}\")\n",
    "\n",
    "# Check Python version\n",
    "import sys\n",
    "python_version = sys.version.split()[0]\n",
    "print(f\"‚úÖ Python Version: {python_version}\")\n",
    "\n",
    "# Verify Spark is working\n",
    "try:\n",
    "    test_df = spark.range(1, 3)\n",
    "    test_count = test_df.count()\n",
    "    print(f\"‚úÖ Spark Context: Active (test count = {test_count})\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Spark Context Error: {e}\")\n",
    "    \n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úÖ ENVIRONMENT VALIDATION COMPLETE\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c825af9b-2b8c-4c65-b910-386b786719f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CELL 1: SNOWFLAKE POLARIS CONFIGURATION\n",
    "# ============================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"üîß STEP 2: CONFIGURING SNOWFLAKE POLARIS CATALOG CONNECTION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Snowflake Polaris credentials - UPDATE WITH YOUR VALUES\n",
    "ACCOUNTADMIN_PAT = \"YOUR_SNOWFLAKE_PAT_TOKEN_HERE\"  # Get from: Snowflake ‚Üí Profile ‚Üí Security ‚Üí Tokens\n",
    "SNOWFLAKE_ACCOUNT = \"YOUR-SNOWFLAKE-ACCOUNT\"  # Format: ORGNAME-ACCOUNTNAME (no underscores!)\n",
    "DATABASE_NAME = \"DEMO_TESTDB\"\n",
    "horizon_role = \"session:role:ACCOUNTADMIN\"\n",
    "\n",
    "# Construct Polaris API endpoint\n",
    "account_uri = f\"https://{SNOWFLAKE_ACCOUNT}.snowflakecomputing.com/polaris/api/catalog\"\n",
    "\n",
    "print(f\"\\nüìç Configuration Details:\")\n",
    "print(f\"   Account: {SNOWFLAKE_ACCOUNT}\")\n",
    "print(f\"   Database: {DATABASE_NAME}\")\n",
    "print(f\"   Polaris URI: {account_uri}\")\n",
    "print(f\"   Role: ACCOUNTADMIN\")\n",
    "\n",
    "# Configure Spark to use Snowflake Polaris as Iceberg catalog\n",
    "print(f\"\\n‚öôÔ∏è  Configuring Spark catalog 'hz' for Polaris...\")\n",
    "\n",
    "try:\n",
    "    # Clear any existing catalog configuration first\n",
    "    try:\n",
    "        spark.conf.unset(\"spark.sql.catalog.hz\")\n",
    "        spark.conf.unset(\"spark.sql.catalog.hz.type\")\n",
    "    except:\n",
    "        pass  # Ignore if not set\n",
    "    \n",
    "    # Core Polaris/Iceberg configuration\n",
    "    # Use catalog-impl approach (not type) to avoid conflicts\n",
    "    spark.conf.set(\"spark.sql.catalog.hz\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "    spark.conf.set(\"spark.sql.catalog.hz.catalog-impl\", \"org.apache.iceberg.rest.RESTCatalog\")\n",
    "    spark.conf.set(\"spark.sql.catalog.hz.uri\", account_uri)\n",
    "    spark.conf.set(\"spark.sql.catalog.hz.warehouse\", DATABASE_NAME)\n",
    "    spark.conf.set(\"spark.sql.catalog.hz.credential\", ACCOUNTADMIN_PAT)\n",
    "    spark.conf.set(\"spark.sql.catalog.hz.scope\", horizon_role)\n",
    "    spark.conf.set(\"spark.sql.catalog.hz.header.X-Snowflake-Authorization-Token-Type\", \"PROGRAMMATIC_ACCESS_TOKEN\")\n",
    "    spark.conf.set(\"spark.sql.iceberg.vectorization.enabled\", \"false\")\n",
    "    \n",
    "    print(\"‚úÖ Spark catalog configuration set successfully\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Configuration Error: {e}\")\n",
    "    raise\n",
    "\n",
    "# Verify configuration was applied\n",
    "print(f\"\\nüîç Verifying configuration...\")\n",
    "catalog_uri = spark.conf.get(\"spark.sql.catalog.hz.uri\")\n",
    "catalog_warehouse = spark.conf.get(\"spark.sql.catalog.hz.warehouse\")\n",
    "print(f\"   ‚úÖ Catalog URI: {catalog_uri}\")\n",
    "print(f\"   ‚úÖ Warehouse: {catalog_warehouse}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úÖ POLARIS CONFIGURATION COMPLETE\")\n",
    "print(\"=\" * 80)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "492621e2-2edb-4899-9c27-b8869f6ae1d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CELL 2: TEST CONNECTION & DISCOVERY\n",
    "# ============================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"üîß STEP 3: TESTING POLARIS CONNECTION & DISCOVERING SCHEMAS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Test 1: List namespaces (schemas)\n",
    "print(\"\\nüìã Test 1: Listing namespaces in Polaris catalog...\")\n",
    "try:\n",
    "    namespaces_df = spark.sql(\"SHOW NAMESPACES IN hz\")\n",
    "    namespace_count = namespaces_df.count()\n",
    "    \n",
    "    print(f\"‚úÖ Successfully connected to Polaris catalog!\")\n",
    "    print(f\"‚úÖ Found {namespace_count} namespace(s):\\n\")\n",
    "    namespaces_df.show(truncate=False)\n",
    "    \n",
    "    # Store namespaces for validation\n",
    "    # Note: The column might be 'databaseName' or 'namespace' depending on Iceberg version\n",
    "    try:\n",
    "        namespaces = [row.databaseName for row in namespaces_df.collect()]\n",
    "    except:\n",
    "        namespaces = [row.namespace for row in namespaces_df.collect()]\n",
    "    \n",
    "    # Verify PUBLIC schema exists\n",
    "    if \"PUBLIC\" in namespaces:\n",
    "        print(\"‚úÖ PUBLIC schema found (expected)\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  PUBLIC schema not found - check Snowflake setup\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå CONNECTION FAILED: {e}\")\n",
    "    print(\"\\nüí° Troubleshooting:\")\n",
    "    print(\"   1. Verify Polaris catalog is enabled in Snowflake\")\n",
    "    print(\"   2. Check your PAT token is valid\")\n",
    "    print(\"   3. Ensure database DEMO_TESTDB exists\")\n",
    "    print(\"   4. Verify Iceberg libraries are installed on cluster\")\n",
    "    raise\n",
    "\n",
    "# Test 2: List tables in PUBLIC schema\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üìã Test 2: Listing tables in PUBLIC schema...\")\n",
    "try:\n",
    "    tables_df = spark.sql(\"SHOW TABLES IN hz.PUBLIC\")\n",
    "    table_count = tables_df.count()\n",
    "    \n",
    "    print(f\"‚úÖ Found {table_count} table(s) in PUBLIC schema:\\n\")\n",
    "    tables_df.show(truncate=False)\n",
    "    \n",
    "    # Store table names for validation\n",
    "    # Get the first row to check available columns\n",
    "    if table_count > 0:\n",
    "        first_row = tables_df.first()\n",
    "        # Try different possible column names\n",
    "        try:\n",
    "            tables = [row.tableName for row in tables_df.collect()]\n",
    "        except:\n",
    "            try:\n",
    "                tables = [row.name for row in tables_df.collect()]\n",
    "            except:\n",
    "                # Just get all column values from first column\n",
    "                tables = [row[0] for row in tables_df.collect()]\n",
    "        \n",
    "        # Verify USER_INFO table exists\n",
    "        tables_upper = [t.upper() if t else \"\" for t in tables]\n",
    "        if \"USER_INFO\" in tables_upper:\n",
    "            print(\"‚úÖ USER_INFO table found (target table confirmed)\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è  USER_INFO table not found - check Snowflake table creation\")\n",
    "            print(f\"   Available tables: {tables}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  No tables found in PUBLIC schema\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå DISCOVERY FAILED: {e}\")\n",
    "    raise\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úÖ CONNECTION TEST & DISCOVERY COMPLETE\")\n",
    "print(\"=\" * 80)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b6b1097-97fc-4897-9744-5ad3bbd55ada",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CELL 3: QUERY USER_INFO TABLE\n",
    "# ============================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"üîß STEP 4: READING USER_INFO TABLE FROM SNOWFLAKE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Query the USER_INFO table\n",
    "print(\"\\nüë• Querying USER_INFO table via Polaris catalog...\")\n",
    "\n",
    "user_info_df = spark.sql(\"SELECT * FROM hz.PUBLIC.USER_INFO\")\n",
    "\n",
    "print(\"\\nüìä USER_INFO Table:\")\n",
    "user_info_df.show(truncate=False)\n",
    "\n",
    "# Show row count\n",
    "row_count = user_info_df.count()\n",
    "print(f\"\\n‚úÖ Total rows: {row_count}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úÖ QUERY COMPLETE\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CELL 4: BI-DIRECTIONAL FEDERATION - WRITE TO SNOWFLAKE\n",
    "# ============================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"STEP 4: WRITING DATA BACK TO SNOWFLAKE VIA POLARIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Test 1: Insert new data\n",
    "print(\"\\nDemo 1: INSERT - Adding new users from Databricks...\")\n",
    "try:\n",
    "    spark.sql(\"\"\"\n",
    "        INSERT INTO hz.PUBLIC.USER_INFO \n",
    "        VALUES \n",
    "            ('databricks_user', 'dbx@databricks.com'),\n",
    "            ('spark_analyst', 'analyst@spark.com')\n",
    "    \"\"\")\n",
    "    \n",
    "    print(\"SUCCESS: Insert successful! Data written to Snowflake.\")\n",
    "    \n",
    "    # Verify the insert\n",
    "    print(\"\\nUpdated table contents:\")\n",
    "    updated_df = spark.sql(\"SELECT * FROM hz.PUBLIC.USER_INFO ORDER BY username\")\n",
    "    updated_df.show(truncate=False)\n",
    "    \n",
    "    new_count = updated_df.count()\n",
    "    print(\"\\nNew row count: \" + str(new_count) + \" (was 4, added 2)\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"ERROR: Insert failed: \" + str(e))\n",
    "    print(\"TIP: Ensure PAT token has INSERT privileges\")\n",
    "\n",
    "# Test 2: Create analytics table\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Demo 2: CREATE TABLE - Building analytics from Databricks...\")\n",
    "try:\n",
    "    # Create aggregated results table\n",
    "    spark.sql(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS hz.PUBLIC.USER_ANALYTICS (\n",
    "            analysis_timestamp TIMESTAMP,\n",
    "            total_users INT,\n",
    "            unique_domains INT,\n",
    "            top_domain STRING,\n",
    "            created_by STRING\n",
    "        )\n",
    "        USING iceberg\n",
    "    \"\"\")\n",
    "    \n",
    "    print(\"SUCCESS: Analytics table created!\")\n",
    "    \n",
    "    # Calculate and insert analytics\n",
    "    user_df = spark.sql(\"SELECT * FROM hz.PUBLIC.USER_INFO\")\n",
    "    \n",
    "    total_users = user_df.count()\n",
    "    domains_df = user_df.withColumn(\"domain\", F.split(F.col(\"EMAIL\"), \"@\").getItem(1))\n",
    "    unique_domains = domains_df.select(\"domain\").distinct().count()\n",
    "    top_domain = domains_df.groupBy(\"domain\").count().orderBy(F.desc(\"count\")).first()[\"domain\"]\n",
    "    \n",
    "    # Insert analytics results\n",
    "    spark.sql(f\"\"\"\n",
    "        INSERT INTO hz.PUBLIC.USER_ANALYTICS VALUES (\n",
    "            CURRENT_TIMESTAMP(),\n",
    "            {total_users},\n",
    "            {unique_domains},\n",
    "            '{top_domain}',\n",
    "            'Databricks Spark'\n",
    "        )\n",
    "    \"\"\")\n",
    "    \n",
    "    print(\"\\nAnalytics results written to Snowflake:\")\n",
    "    analytics_df = spark.sql(\"SELECT * FROM hz.PUBLIC.USER_ANALYTICS\")\n",
    "    analytics_df.show(truncate=False)\n",
    "    \n",
    "    print(\"\\nTIP: Go check Snowflake - this table is visible there now!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"ERROR: Failed: \" + str(e))\n",
    "    print(\"TIP: Check CREATE TABLE privileges\")\n",
    "\n",
    "# Test 3: Update existing data\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Demo 3: UPDATE - Modifying data from Databricks...\")\n",
    "try:\n",
    "    spark.sql(\"\"\"\n",
    "        UPDATE hz.PUBLIC.USER_INFO \n",
    "        SET EMAIL = 'updated@databricks.com'\n",
    "        WHERE username = 'databricks_user'\n",
    "    \"\"\")\n",
    "    \n",
    "    print(\"SUCCESS: Update successful!\")\n",
    "    \n",
    "    # Show the updated record\n",
    "    print(\"\\nUpdated record:\")\n",
    "    spark.sql(\"\"\"\n",
    "        SELECT * FROM hz.PUBLIC.USER_INFO \n",
    "        WHERE username = 'databricks_user'\n",
    "    \"\"\").show(truncate=False)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"ERROR: Update failed: \" + str(e))\n",
    "\n",
    "# Test 4: DataFrame API write\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Demo 4: DATAFRAME API - Writing using Spark DataFrames...\")\n",
    "try:\n",
    "    # Create new data using DataFrame API\n",
    "    new_users = spark.createDataFrame([\n",
    "        (\"ml_engineer\", \"ml@company.com\"),\n",
    "        (\"data_scientist\", \"ds@company.com\")\n",
    "    ], [\"USERNAME\", \"EMAIL\"])\n",
    "    \n",
    "    print(\"Writing 2 more users using DataFrame API...\")\n",
    "    \n",
    "    # Write using DataFrameWriter\n",
    "    new_users.writeTo(\"hz.PUBLIC.USER_INFO\").append()\n",
    "    \n",
    "    print(\"SUCCESS: DataFrame write successful!\")\n",
    "    \n",
    "    # Show final results\n",
    "    print(\"\\nFinal table state:\")\n",
    "    final_df = spark.sql(\"SELECT * FROM hz.PUBLIC.USER_INFO ORDER BY username\")\n",
    "    final_df.show(truncate=False)\n",
    "    \n",
    "    final_count = final_df.count()\n",
    "    print(\"\\nFinal count: \" + str(final_count) + \" users\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"ERROR: DataFrame write failed: \" + str(e))\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"BI-DIRECTIONAL FEDERATION DEMONSTRATION COMPLETE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nWhat We Demonstrated:\")\n",
    "print(\"  - INSERT: Added new records from Databricks\")\n",
    "print(\"  - CREATE TABLE: Built analytics table in Snowflake\")\n",
    "print(\"  - UPDATE: Modified existing Snowflake data\")\n",
    "print(\"  - DataFrame API: Used Spark native writes\")\n",
    "\n",
    "print(\"\\nKey Takeaways:\")\n",
    "print(\"  - All changes are IMMEDIATELY visible in Snowflake\")\n",
    "print(\"  - Iceberg ensures ACID transaction guarantees\")\n",
    "print(\"  - True bi-directional federation (not just read-only)\")\n",
    "print(\"  - Process in Databricks, store in Snowflake - best of both!\")\n",
    "\n",
    "print(\"\\nNow check Snowflake to verify:\")\n",
    "print(\"  SELECT * FROM DEMO_TESTDB.PUBLIC.USER_INFO;\")\n",
    "print(\"  SELECT * FROM DEMO_TESTDB.PUBLIC.USER_ANALYTICS;\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CELL 4: FINAL VALIDATION & DEMO SUMMARY\n",
    "# ============================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"üéØ FINAL VALIDATION & DEMO SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Summary of what was accomplished\n",
    "print(\"\\n‚úÖ DEMO COMPLETED SUCCESSFULLY!\\n\")\n",
    "\n",
    "print(\"üìã What We Accomplished:\")\n",
    "print(\"   ‚úÖ Step 1: Validated Databricks environment\")\n",
    "print(\"   ‚úÖ Step 2: Configured Snowflake Polaris catalog connection\")\n",
    "print(\"   ‚úÖ Step 3: Tested connection and discovered schemas/tables\")\n",
    "print(\"   ‚úÖ Step 4: Queried USER_INFO table from Snowflake\")\n",
    "print(\"   ‚úÖ Step 5: Wrote data back to Snowflake (bi-directional!)\")\n",
    "\n",
    "print(\"\\nüèóÔ∏è  Architecture Demonstrated:\")\n",
    "print(\"   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\")\n",
    "print(\"   ‚îÇ         SNOWFLAKE                        ‚îÇ\")\n",
    "print(\"   ‚îÇ  Database: DEMO_TESTDB                   ‚îÇ\")\n",
    "print(\"   ‚îÇ  Table: USER_INFO (Iceberg format)       ‚îÇ\")\n",
    "print(\"   ‚îÇ  Polaris Catalog (REST API)              ‚îÇ\")\n",
    "print(\"   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\")\n",
    "print(\"                  ‚îÇ\")\n",
    "print(\"                  ‚îÇ REST API over HTTPS\")\n",
    "print(\"                  ‚îÇ (Iceberg protocol)\")\n",
    "print(\"                  ‚îÇ\")\n",
    "print(\"   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\")\n",
    "print(\"   ‚îÇ         DATABRICKS                       ‚îÇ\")\n",
    "print(\"   ‚îÇ  Spark + Iceberg libraries               ‚îÇ\")\n",
    "print(\"   ‚îÇ  Querying remote Iceberg tables          ‚îÇ\")\n",
    "print(\"   ‚îÇ  No data duplication!                    ‚îÇ\")\n",
    "print(\"   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\")\n",
    "\n",
    "print(\"\\nüîë Key Capabilities Shown:\")\n",
    "print(\"   ‚Ä¢ Data Federation: Query Snowflake data from Databricks\")\n",
    "print(\"   ‚Ä¢ Bi-Directional Writes: Write data back to Snowflake\")\n",
    "print(\"   ‚Ä¢ Open Standards: Using Apache Iceberg format\")\n",
    "print(\"   ‚Ä¢ No ETL: Direct access without copying data\")\n",
    "print(\"   ‚Ä¢ Polaris Catalog: Snowflake's REST-based catalog service\")\n",
    "print(\"   ‚Ä¢ ACID Transactions: Guaranteed data consistency\")\n",
    "\n",
    "print(\"\\nüìä Table Statistics:\")\n",
    "try:\n",
    "    final_count = spark.sql(\"SELECT COUNT(*) as count FROM hz.PUBLIC.USER_INFO\").collect()[0]['count']\n",
    "    print(f\"   Total rows in USER_INFO: {final_count}\")\n",
    "    \n",
    "    final_sample = spark.sql(\"SELECT * FROM hz.PUBLIC.USER_INFO LIMIT 3\")\n",
    "    print(\"\\n   Sample data:\")\n",
    "    final_sample.show(truncate=False)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"   Could not fetch final stats: {e}\")\n",
    "\n",
    "print(\"\\nüöÄ Next Steps - Phase 2:\")\n",
    "print(\"   1. Add more tables (CUSTOMERS, ORDERS, PRODUCTS)\")\n",
    "print(\"   2. Set up Snowflake Cortex Analyst (semantic model)\")\n",
    "print(\"   3. Create Cortex Agent for natural language queries\")\n",
    "print(\"   4. Build demo presentation and talking points\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úÖ DEMO READY FOR PRESENTATION!\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create a flag to indicate success\n",
    "print(\"\\nüíæ Saving connection state for future use...\")\n",
    "spark.conf.set(\"demo.polaris.connection.validated\", \"true\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CELL 5: TROUBLESHOOTING & DIAGNOSTICS (Optional)\n",
    "# ============================================\n",
    "# Run this cell only if you encounter errors in previous cells\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"üîß TROUBLESHOOTING & DIAGNOSTICS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nüîç Diagnostic Information:\\n\")\n",
    "\n",
    "# Check 1: Verify Iceberg library is installed\n",
    "print(\"1Ô∏è‚É£ Checking for Iceberg libraries...\")\n",
    "try:\n",
    "    iceberg_class = spark.sparkContext._jvm.org.apache.iceberg.spark.SparkCatalog\n",
    "    print(\"   ‚úÖ Iceberg libraries are installed\")\n",
    "except Exception as e:\n",
    "    print(\"   ‚ùå Iceberg libraries NOT found!\")\n",
    "    print(\"   üí° Install: org.apache.iceberg:iceberg-spark-runtime-3.4_2.12:1.4.3\")\n",
    "    print(f\"   Error: {e}\")\n",
    "\n",
    "# Check 2: Verify catalog configuration\n",
    "print(\"\\n2Ô∏è‚É£ Checking catalog configuration...\")\n",
    "try:\n",
    "    catalog_impl = spark.conf.get(\"spark.sql.catalog.hz.catalog-impl\")\n",
    "    catalog_uri = spark.conf.get(\"spark.sql.catalog.hz.uri\")\n",
    "    print(f\"   ‚úÖ Catalog implementation: {catalog_impl}\")\n",
    "    print(f\"   ‚úÖ Catalog URI: {catalog_uri}\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Catalog not configured: {e}\")\n",
    "\n",
    "# Check 3: Network connectivity test\n",
    "print(\"\\n3Ô∏è‚É£ Testing network connectivity to Snowflake...\")\n",
    "try:\n",
    "    import urllib.request\n",
    "    import ssl\n",
    "    \n",
    "    # Test basic HTTPS connectivity to Snowflake\n",
    "    context = ssl.create_default_context()\n",
    "    url = f\"https://{SNOWFLAKE_ACCOUNT}.snowflakecomputing.com\"\n",
    "    \n",
    "    try:\n",
    "        req = urllib.request.Request(url, method='HEAD')\n",
    "        response = urllib.request.urlopen(req, context=context, timeout=10)\n",
    "        print(f\"   ‚úÖ Can reach Snowflake at {url}\")\n",
    "        print(f\"   HTTP Status: {response.status}\")\n",
    "    except Exception as net_err:\n",
    "        print(f\"   ‚ö†Ô∏è  Network issue: {net_err}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"   ‚ö†Ô∏è  Could not test network: {e}\")\n",
    "\n",
    "# Check 4: PAT token format validation\n",
    "print(\"\\n4Ô∏è‚É£ Validating PAT token format...\")\n",
    "try:\n",
    "    if ACCOUNTADMIN_PAT and len(ACCOUNTADMIN_PAT) > 50:\n",
    "        print(\"   ‚úÖ PAT token appears to be set (length > 50 chars)\")\n",
    "        # Check if it looks like a JWT\n",
    "        if ACCOUNTADMIN_PAT.count('.') == 2:\n",
    "            print(\"   ‚úÖ PAT token format looks like a JWT (3 parts)\")\n",
    "        else:\n",
    "            print(\"   ‚ö†Ô∏è  PAT token doesn't look like a JWT\")\n",
    "    else:\n",
    "        print(\"   ‚ùå PAT token not set or too short\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå PAT token validation failed: {e}\")\n",
    "\n",
    "# Check 5: List all Spark catalog configurations\n",
    "print(\"\\n5Ô∏è‚É£ All Spark catalog configurations:\")\n",
    "try:\n",
    "    all_configs = spark.sparkContext.getConf().getAll()\n",
    "    catalog_configs = [conf for conf in all_configs if 'catalog.hz' in conf[0]]\n",
    "    \n",
    "    if catalog_configs:\n",
    "        for key, value in catalog_configs:\n",
    "            # Mask the credential\n",
    "            if 'credential' in key:\n",
    "                value = value[:20] + \"...\" if len(value) > 20 else \"***\"\n",
    "            print(f\"   {key} = {value}\")\n",
    "    else:\n",
    "        print(\"   ‚ö†Ô∏è  No catalog configurations found\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Could not list configurations: {e}\")\n",
    "\n",
    "# Check 6: Common error patterns\n",
    "print(\"\\n6Ô∏è‚É£ Common Issues & Solutions:\")\n",
    "print(\"   ‚ùå 'Cannot find catalog plugin' ‚Üí Install Iceberg libraries\")\n",
    "print(\"   ‚ùå 'RESTException' ‚Üí Check Polaris is enabled in Snowflake\")\n",
    "print(\"   ‚ùå '401 Unauthorized' ‚Üí Verify PAT token is valid\")\n",
    "print(\"   ‚ùå '404 Not Found' ‚Üí Check database name and Snowflake account\")\n",
    "print(\"   ‚ùå 'Connection refused' ‚Üí Network/firewall issues\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úÖ DIAGNOSTICS COMPLETE\")\n",
    "print(\"=\" * 80)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Storage for Iceberg demo - trial account",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
