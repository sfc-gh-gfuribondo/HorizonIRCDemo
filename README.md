# Snowflake Polaris + Databricks Iceberg Demo

This repository demonstrates data federation between Snowflake and Databricks using Apache Iceberg tables and Snowflake's Polaris catalog.

## ğŸ¯ Demo Overview

**What This Demonstrates:**
- âœ… Snowflake hosting Iceberg tables with external volume
- âœ… Databricks querying Snowflake data via Polaris REST API
- âœ… Data federation without duplication
- âœ… Open table format (Apache Iceberg) for interoperability

**Architecture:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         SNOWFLAKE                        â”‚
â”‚  Database: DEMO_TESTDB                   â”‚
â”‚  Table: USER_INFO (Iceberg format)       â”‚
â”‚  Polaris Catalog (REST API)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â”‚ REST API over HTTPS
               â”‚ (Iceberg protocol)
               â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         DATABRICKS                       â”‚
â”‚  Spark + Iceberg libraries               â”‚
â”‚  Querying remote Iceberg tables          â”‚
â”‚  No data duplication!                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“‹ Prerequisites

### Snowflake Requirements:
- Snowflake account (Enterprise Edition or trial with Polaris enabled)
- ACCOUNTADMIN role access
- Polaris catalog feature enabled
- PrPr features enabled (snowflake managed storage, horizon polaris endpoint)

### Databricks Requirements:
- Databricks workspace
- Cluster with Apache Iceberg libraries installed
- Network access to Snowflake

## ğŸš€ Quick Start

### Step 1: Set Up Snowflake

1. Log into your Snowflake account with ACCOUNTADMIN role
2. Open a SQL worksheet
3. Run the setup script:
   ```bash
   # In Snowflake SQL worksheet, run:
   snowflake/setup_iceberg_tables.sql
   ```

This will:
- Create `DEMO_TESTDB` database
- Set up external volume for Iceberg storage
- Create `USER_INFO` Iceberg table with sample data

### Step 2: Generate Snowflake PAT Token

1. In Snowflake, go to your user profile (top right)
2. Navigate to **My Profile â†’ Security â†’ Personal Access Tokens**
3. Click **+ Token**
4. Name it "Databricks_Demo"
5. Copy the token (save it securely!)

### Step 3: Set Up Databricks Cluster

1. Create a new cluster or use existing one
2. Install Iceberg library:
   - Go to cluster â†’ **Libraries** â†’ **Install New**
   - Select **Maven**
   - Coordinates: `org.apache.iceberg:iceberg-spark-runtime-3.4_2.12:1.4.3`
     (adjust version based on your DBR version - see config/databricks_cluster_config.json)
3. **Restart the cluster** after installation

### Step 4: Configure and Run Notebook

1. Import the notebook from `notebooks/polaris_iceberg_demo.ipynb`
2. Update Cell 1 with your credentials:
   ```python
   ACCOUNTADMIN_PAT = "your_pat_token_here"
   SNOWFLAKE_ACCOUNT = "your_account_identifier"
   ```
3. Run cells in order (0 â†’ 1 â†’ 2 â†’ 3 â†’ 4)

## ğŸ“ Repository Structure

```
OptumJamDemo/
â”œâ”€â”€ README.md                           # This file
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ polaris_iceberg_demo.ipynb     # Main demo notebook
â”œâ”€â”€ snowflake/
â”‚   â””â”€â”€ setup_iceberg_tables.sql       # Snowflake table setup
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ databricks_cluster_config.json # Cluster configuration template
â”‚   â””â”€â”€ library_requirements.txt       # Maven library coordinates
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ DEMO_GUIDE.md                  # Presentation guide
â”‚   â””â”€â”€ TROUBLESHOOTING.md             # Common issues
â””â”€â”€ .gitignore                         # Git ignore patterns
```

## ğŸ”‘ Key Files

### Notebooks
- **`polaris_iceberg_demo.ipynb`**: Complete Databricks notebook with:
  - Environment validation
  - Polaris connection configuration
  - Table discovery
  - Query execution
  - Diagnostics

### Snowflake Scripts
- **`setup_iceberg_tables.sql`**: Creates Iceberg infrastructure in Snowflake

### Configuration
- **`databricks_cluster_config.json`**: Cluster specs and library requirements
- **`library_requirements.txt`**: Maven coordinates for Iceberg libraries

## ğŸ“ Demo Flow

1. **Cell 0**: Validate Databricks environment
2. **Cell 1**: Configure Snowflake Polaris connection
3. **Cell 2**: Test connection and discover tables
4. **Cell 3**: Query USER_INFO table
5. **Cell 4**: Summary and next steps
6. **Cell 5**: (Optional) Diagnostics if issues occur

## ğŸ”§ Troubleshooting

### Common Issues:

**âŒ "Cannot find catalog plugin"**
- **Solution**: Install Iceberg libraries on cluster
- Maven: `org.apache.iceberg:iceberg-spark-runtime-3.4_2.12:1.4.3`

**âŒ "RESTException" or "401 Unauthorized"**
- **Solution**: Verify Polaris is enabled and PAT token is valid

**âŒ "Contains non-LDH ASCII characters"**
- **Solution**: Check Snowflake account identifier format
- Use hyphens, not underscores: `ORGNAME-ACCOUNTNAME`

**âŒ "Attribute not supported"**
- **Solution**: Updated code handles different column names in Iceberg versions

See `docs/TROUBLESHOOTING.md` for detailed solutions.

## ğŸ“Š What's Next (Phase 2)?

After completing this demo:
1. Add more tables (CUSTOMERS, ORDERS, PRODUCTS)
2. Set up Snowflake Cortex Analyst (semantic model)
3. Create Cortex Agent for natural language queries
4. Build interactive demo presentation

## ğŸ”’ Security Notes

- **Never commit PAT tokens** to version control
- Use Databricks secrets or environment variables in production
- The `.gitignore` file excludes sensitive files
- PAT tokens expire - check expiration date

## ğŸ“ Configuration Notes

### Finding Your Snowflake Account Identifier:

Run this in Snowflake:
```sql
SELECT CURRENT_ACCOUNT();
SELECT CURRENT_ORGANIZATION_NAME();
```

The account identifier format:
- Format: `ORGNAME-ACCOUNTNAME`
- Example: `SFSENORTHAMERICA-DEMOGFURIBONDO`
- **Use hyphens, not underscores!**

### Databricks Runtime Versions:

| DBR Version | Iceberg Library |
|-------------|-----------------|
| DBR 12.x (Spark 3.3) | `org.apache.iceberg:iceberg-spark-runtime-3.3_2.12:1.4.3` |
| DBR 13.x, 14.x (Spark 3.4) | `org.apache.iceberg:iceberg-spark-runtime-3.4_2.12:1.4.3` |
| DBR 15.x (Spark 3.5) | `org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.0` |

## ğŸ“š Additional Resources

- [Apache Iceberg Documentation](https://iceberg.apache.org/)
- [Snowflake Polaris Catalog](https://docs.snowflake.com/en/user-guide/polaris/index.html)
- [Databricks Iceberg Integration](https://docs.databricks.com/en/delta/uniform.html)

## ğŸ‘¥ Support

For issues or questions:
1. Check `docs/TROUBLESHOOTING.md`
2. Run Cell 5 (Diagnostics) in the notebook
3. Review Snowflake and Databricks documentation

## ğŸ“„ License

This demo code is provided as-is for educational and demonstration purposes.

---

**Ready to start?** Follow the Quick Start steps above! ğŸš€

